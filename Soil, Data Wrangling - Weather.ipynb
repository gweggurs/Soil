{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f110cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IT'S DANGEROUS TO GO ALONE! TAKE THIS:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#comebacktothis wherever I could improve or left something ugly\n",
    "#gethelp wherever I need it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9571c02",
   "metadata": {},
   "source": [
    "*In order to give my computer's RAM a break, I broke the wrangling/cleaning up into three notebooks.\n",
    "This is part one of the miniseries - getting the data from the United States' National Oceanic and Atmospheric Administration reformatted, cleaned, and into one single dataframe for the 20 years of data we will be working on.*\n",
    "\n",
    "*On that note, the 'rough' csv's ('roughMinTemp.csv', etc) have been zipped and transferred to the coding storage folder on my D drive, in the Capstone subfolder:*\n",
    "* D:\\gdsak\\Coding_Storage\\Capstone\\roughWeathers*\n",
    "\n",
    "*And the cleaned csv's ('avg_temp.csv', etc) are also zipped together in the same location:*\n",
    "* D:\\gdsak\\Coding_Storage\\Capstone\\cleanWeathers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30e2db",
   "metadata": {},
   "source": [
    "# NOAA weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf6d6b",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "We have three data sources, each with slightly different types of data. The goal is to have either weekly (with imputed/interpolated weather data) or monthly (with binned and averaged drought data)\n",
    "\n",
    "- **NOAA Weather data** - We'll focus on this\n",
    "    * This is 4 separate text files. I need to slice out the number that signifies what the data *is* from the fips/year column for each dataframe. Then pivot the rest of the columns into two vertical columns for month and precip/avg temp/min temp/max temp instead of the 12 columns (one per month) format I have now. Then I should be able to combine the 4 dataframes using the fips/year column as an index, ad drop the extra month columns\n",
    "\n",
    "- US Drought monitor\n",
    "\n",
    "- USDA/NASS Census and Survey of Ag data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51657421",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2925d6ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/roughMinTemp.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18236/811178507.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmin_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/roughMinTemp.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmin_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/roughMinTemp.csv'"
     ]
    }
   ],
   "source": [
    "min_temp = pd.read_csv('data/roughMinTemp.csv')\n",
    "min_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fcd3c9",
   "metadata": {},
   "source": [
    "# Test Drive #1:\n",
    "### Getting the Weather data into tidy columns.\n",
    "\n",
    "My weather data started out as a text file that I converted to .csv in the acquisition notebook. I started with 13 columns:\n",
    "* The first column was for the state/county FIPS # combined with a two digit number indicating the type of data and the year in YYYY format.\n",
    "\n",
    "* The next 12 columns were for each month of the year.\n",
    "\n",
    "What I want is a 'tidy' dataframe. The notion of tidy data came from an introductory book on **R***. The goal of tidying up one's data is to have your data following these three rules:\n",
    "\n",
    "1. Each variable must have its own column\n",
    "2. Each observation must have its own row\n",
    "3. Each value must have its own cell\n",
    "\n",
    "This is accomplished with a handful of functions from the tidyverse library. For my purposes the **gather** and **spread** functions are most salient. \n",
    "\n",
    "**Gather** will take a number of rows that are holding the same variable and pivot them vertically. In my case, gathering all of the months and creating a single 'month' column and a single 'min_temp' column (or max_temp/precip, etc). Knowing that this function is commonplace in R led me to search for examples of code that accomplishes this same task in pandas. As has thusfar been the case, if I thought of something I'd like to code, someone has *definitely* already written that code.\n",
    "\n",
    "*R for Data Science by Hadley Wickham & Garrett Grolemund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aaf3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test driving a gather function a la from someone else:\n",
    "# https://gist.github.com/derekpowell/5f97dabdd0730e68380fa1a00cd34ac4\n",
    "\n",
    "def gather( df, key, value, cols ):\n",
    "    id_vars = [ col for col in df.columns if col not in cols ]\n",
    "    id_values = cols\n",
    "    var_name = key\n",
    "    value_name = value\n",
    "    return pd.melt( df, id_vars, id_values, var_name, value_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a 5 row test set for translations and column splitting\n",
    "min_temp_small = min_temp[:5].copy()\n",
    "min_temp_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fd62f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "gather(df=min_temp_small,\n",
    "       key='Month',\n",
    "       value='mintemp',\n",
    "       cols=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'])\n",
    "#yep, that works, now lets fix the first column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4645756",
   "metadata": {},
   "source": [
    "Et voila, the borrowed 'gather' function performed as desired. Now to move on to fixing the first column.\n",
    "\n",
    "The 'FIPSXXYear column for all four weather csv's need to be split up into three columns:\n",
    "\n",
    "* FIPS - this is currently either 4 or 5 digits, but *should* be 5 digits for all values. The first two digits are for the state and the last 3 digits for the county in that state. I will need to add a leading zero whenever the FIPS number is only 4 digits long.\n",
    "* 28/27/02/01 - the 2 digit code that signifies what data is held in dataframe. This is a holdover from downloading the data from the NOAA database, I'll be dropping this column.\n",
    "* Year - the year in YYYY format, this column will be kept as is and once I have a tidy dataframe with all four of my weather variables I can slice out a copy with data from 2002 to 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b83c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#setting the fips/year column to be a string\n",
    "min_temp_small['FIPS28Year'] = min_temp_small['FIPS28Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure it worked\n",
    "min_temp_small.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding in the leading zero\n",
    "min_temp_small['FIPS28Year'] = min_temp_small['FIPS28Year'].str.zfill(11)\n",
    "min_temp_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559be857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#after some googling for ways to break up a string with regex (because using '28' as a delimiter would've been bad/wrong)\n",
    "#I found an answer on stackexchange: \n",
    "#https://stackoverflow.com/questions/25252200/how-to-split-a-column-based-on-several-string-indices-using-pandas\n",
    "#and tested it out in regex101 until I got the below code to behave\n",
    "\n",
    "min_temp_small['FIPS28Year'].str.extract('(.{5})(.{2})(.{4})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new columns with the extracted data\n",
    "min_temp_small[['FIPS', '28', 'Year']] = min_temp_small['FIPS28Year'].str.extract('(.{5})(.{2})(.{4})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ccad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking my work\n",
    "min_temp_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9b50d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#adding in the gather function\n",
    "min_temp_small = gather(df=min_temp_small,\n",
    "       key='Month',\n",
    "       value='mintemp',\n",
    "       cols=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'])\n",
    "min_temp_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b346a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#and dropping the FIPS28Year and 28 columns\n",
    "min_temp_small = min_temp_small.drop(columns=['FIPS28Year', '28'])\n",
    "min_temp_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382d9af",
   "metadata": {},
   "source": [
    "The last thing I will need is to create an index column that merges the FIPS, year, and month so it is easy to join the dataframes together into one weather dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19932223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#recombining the FIPS and year columns so I have something to join the columns on\n",
    "min_temp_small['FIPSYearMonth'] = min_temp_small['FIPS'] + min_temp_small['Year'] + min_temp_small['Month']\n",
    "min_temp_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c51f9",
   "metadata": {},
   "source": [
    "Now, to load in the other three sets of weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0df061",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temp = pd.read_csv('data/roughMaxTemp.csv')\n",
    "max_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2701fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_temp = pd.read_csv('data/roughAvgTemp.csv')\n",
    "avg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737aaac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precip = pd.read_csv('data/roughPrecip.csv')\n",
    "precip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6ac86",
   "metadata": {},
   "source": [
    "### Cleaning note:\n",
    "The Temperature data has missing values represented with **-99.99** (an impossibly low temperature in fahrenheit for the contiguous USA), while the precipitation data has missing values represented with **-9.99** (a distinctly possible temperature, one that is unfortunately common in the author's hometown in January and February).\n",
    "\n",
    "Because of this difference in how missing values are presented, we will pre-clean the precipitation data. Because we are mainly interested in the years after 2000, we can wait to do this cleaning until the end of the reformatting of the precip dataframe.\n",
    "\n",
    "Let's start with precip! We have 6 steps to clean up each of the weather dataframes: \n",
    "\n",
    "# COMEBACKTOTHIS \n",
    "maybe turn this heap into a function, eh?\n",
    "\n",
    "**1. Convert to string**\n",
    "    - min_temp_small['FIPS28Year'] = min_temp_small['FIPS28Year'].astype(str)\n",
    "\n",
    "**2. Add leading zeros**\n",
    "    - min_temp_small['FIPS28Year'] = min_temp_small['FIPS28Year'].str.zfill(11)\n",
    "\n",
    "**3. Gather the month columns**\n",
    "    - min_temp_small = gather(df=min_temp_small,\n",
    "       key='Month',\n",
    "       value='mintemp',\n",
    "       cols=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'])\n",
    "\n",
    "**4. Extract and create new columns**\n",
    "    - min_temp_small[['FIPS', '28', 'Year']] = min_temp_small['FIPS28Year'].str.extract('(.{5})(.{2})(.{4})')\n",
    "\n",
    "\n",
    "**5. Drop the old combined column and the unnecessary label column**\n",
    "    - min_temp_small = min_temp_small.drop(columns=['FIPS28Year', '28'])\n",
    "\n",
    "**6. Make an column from the FIPS, Year, and Month so when we join everything we have a key to join on**\n",
    "    - min_temp_small['FIPSYearMonth'] = min_temp_small['FIPS'] + min_temp_small['Year'] + min_temp_small['Month']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7553f",
   "metadata": {},
   "source": [
    "### Reformatting precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afcb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert to string and add leading zeros\n",
    "precip['FIPS01Year'] = precip['FIPS01Year'].astype(str)\n",
    "precip['FIPS01Year'] = precip['FIPS01Year'].str.zfill(11)\n",
    "precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9dbcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather the month data into one column\n",
    "precip = gather(df=precip,\n",
    "       key='Month',\n",
    "       value='precip',\n",
    "       cols=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting and adding newly split-up columns\n",
    "precip[['FIPS', '01', 'Year']] = precip['FIPS01Year'].str.extract('(.{5})(.{2})(.{4})')\n",
    "precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc152196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping the extra columns, adding in the joining column\n",
    "precip = precip.drop(columns=['FIPS01Year', '01'])\n",
    "precip['FIPSYearMonth'] = precip['FIPS'] + precip['Year'] + precip['Month']\n",
    "precip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979857e",
   "metadata": {},
   "source": [
    "### Pre-wash cycle\n",
    "Our precip dataframe is in the right format, now lets check for missing data. If there is quite a bit of it outside the 20 years we are planning on using, we can cut out the chunk of data we are going to use and perform the changes there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "precip[precip['precip']==-9.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49934ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values that aren't in 2022\n",
    "precip[\n",
    "    (precip['precip']==-9.99) & (precip['Year']!='2022')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for nulls\n",
    "precip.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc9504",
   "metadata": {},
   "source": [
    "It seems that the precipitation data is whole, apart from the last five months in 2022-- which makes sense, considering the data was pulled in August of 2022. We can check the unique number of FIPS, it should be ~3,000, and multiply that by 5. If it's close to the total we are seeing for the first search, then we should have complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f768b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"There are {(precip['FIPS'].nunique())} unique FIPS numbers.\\n\\\n",
    "There theoretically are 5 months missing for each, so we should have {(precip['FIPS'].nunique())*5} rows with -9.99.\\n\\\n",
    "That matches the row count above of {precip[precip['precip']==-9.99].shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221d811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precip.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625b47b",
   "metadata": {},
   "source": [
    "Wonderful! The precip dataframe is clean and complete. We can update the data types for Month and Year to integers and create a copy with all of the years from 2002 through 2021. We'll call this precip20. \n",
    "\n",
    "Then we can repeat the reformatting process for the other three dataframes and combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc03e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precip['Month'] = precip['Month'].astype(int)\n",
    "precip['Year'] = precip['Year'].astype(int)\n",
    "precip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92cbce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precip20 = precip[(precip['Year']>2001) & (precip['Year']!=2022)].copy()\n",
    "precip20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc904ef",
   "metadata": {},
   "source": [
    "### Reformatting max_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d40e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the two digit number that splits the FIPS and year values for max_temp\n",
    "max_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b64c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert to string and add leading zeros\n",
    "max_temp['FIPS27Year'] = max_temp['FIPS27Year'].astype(str)\n",
    "max_temp['FIPS27Year'] = max_temp['FIPS27Year'].str.zfill(11)\n",
    "max_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather the month data into one column\n",
    "max_temp = gather(df=max_temp,\n",
    "       key='Month',\n",
    "       value='maxtemp',\n",
    "       cols=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'])\n",
    "max_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting and adding newly split-up columns\n",
    "max_temp[['FIPS', '27', 'Year']] = max_temp['FIPS27Year'].str.extract('(.{5})(.{2})(.{4})')\n",
    "max_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cd1d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dropping the extra columns, adding in the joining column\n",
    "max_temp = max_temp.drop(columns=['FIPS27Year', '27'])\n",
    "max_temp['FIPSYearMonth'] = max_temp['FIPS'] + max_temp['Year'] + max_temp['Month']\n",
    "max_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affb88e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_temp['Month'] = max_temp['Month'].astype(int)\n",
    "max_temp['Year'] = max_temp['Year'].astype(int)\n",
    "max_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f16f2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#setting up a subsegment for 2002 through 2021\n",
    "max_temp20 = max_temp[(max_temp['Year']>2001) & (max_temp['Year']!=2022)].copy()\n",
    "max_temp20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27c25a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for missing values in max_temp20\n",
    "max_temp20[max_temp20['maxtemp']==-99.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a13d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for nulls\n",
    "max_temp20.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39368763",
   "metadata": {},
   "source": [
    "### Reformatting min_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16435fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the two digit number that splits the FIPS and year values for max_temp\n",
    "min_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c4023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert to string and add leading zeros\n",
    "min_temp['FIPS28Year'] = min_temp['FIPS28Year'].astype(str)\n",
    "min_temp['FIPS28Year'] = min_temp['FIPS28Year'].str.zfill(11)\n",
    "min_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891414d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather the month data into one column\n",
    "min_temp = gather(df=min_temp,\n",
    "       key='Month',\n",
    "       value='mintemp',\n",
    "       cols=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'])\n",
    "min_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting and adding newly split-up columns\n",
    "min_temp[['FIPS', '28', 'Year']] = min_temp['FIPS28Year'].str.extract('(.{5})(.{2})(.{4})')\n",
    "min_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8776853",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dropping the extra columns, adding in the joining column\n",
    "min_temp = min_temp.drop(columns=['FIPS28Year', '28'])\n",
    "min_temp['FIPSYearMonth'] = min_temp['FIPS'] + min_temp['Year'] + min_temp['Month']\n",
    "min_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b6942",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_temp['Month'] = min_temp['Month'].astype(int)\n",
    "min_temp['Year'] = min_temp['Year'].astype(int)\n",
    "min_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f949d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#setting up a subsegment for 2002 through 2021\n",
    "min_temp20 = min_temp[(min_temp['Year']>2001) & (min_temp['Year']!=2022)].copy()\n",
    "min_temp20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b49d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for missing values in min_temp20\n",
    "min_temp20[min_temp20['mintemp']==-99.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for nulls\n",
    "min_temp20.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75fd30",
   "metadata": {},
   "source": [
    "### Reformatting avg_temp\n",
    "##### last but not least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cff4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the two digit number that splits the FIPS and year values for max_temp\n",
    "avg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d01723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert to string and add leading zeros\n",
    "avg_temp['FIPS02Year'] = avg_temp['FIPS02Year'].astype(str)\n",
    "avg_temp['FIPS02Year'] = avg_temp['FIPS02Year'].str.zfill(11)\n",
    "avg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather the month data into one column\n",
    "avg_temp = gather(df=avg_temp,\n",
    "       key='Month',\n",
    "       value='avgtemp',\n",
    "       cols=['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'])\n",
    "avg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d02f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting and adding newly split-up columns\n",
    "avg_temp[['FIPS', '02', 'Year']] = avg_temp['FIPS02Year'].str.extract('(.{5})(.{2})(.{4})')\n",
    "avg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fae93e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dropping the extra columns, adding in the joining column\n",
    "avg_temp = avg_temp.drop(columns=['FIPS02Year', '02'])\n",
    "avg_temp['FIPSYearMonth'] = avg_temp['FIPS'] + avg_temp['Year'] + avg_temp['Month']\n",
    "avg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487d544",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_temp['Month'] = avg_temp['Month'].astype(int)\n",
    "avg_temp['Year'] = avg_temp['Year'].astype(int)\n",
    "avg_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0444b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#setting up a subsegment for 2002 through 2021\n",
    "avg_temp20 = avg_temp[(avg_temp['Year']>2001) & (avg_temp['Year']!=2022)].copy()\n",
    "avg_temp20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6ae84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for missing values in avg_temp20\n",
    "avg_temp20[avg_temp20['avgtemp']==-99.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for nulls\n",
    "avg_temp20.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75431571",
   "metadata": {},
   "source": [
    "I now have four, tidied and abridged dataframes that can be combined into one dataframe, **weather**.\n",
    "I have a column set up in each dataframe that can be used as a key for merging them, so I can actually drop the Month, FIPS, and Year columns from most of the dataframes. Luckily, I don't have missing data for any of the dataframes I will use for our analysis.\n",
    "\n",
    "I'm going to keep precip20 whole and drop the columns from the temperature dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the unnecessary columns\n",
    "avg_temp20 = avg_temp20.drop(columns=['Month', 'Year', 'FIPS'])\n",
    "min_temp20 = min_temp20.drop(columns=['Month', 'Year', 'FIPS'])\n",
    "max_temp20 = max_temp20.drop(columns=['Month', 'Year', 'FIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining precip and avg_temp dataframes on the 'FIPSYearMonth' columns\n",
    "weather = pd.merge(precip20, avg_temp20, on='FIPSYearMonth')\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the min temp dataframe\n",
    "weather = pd.merge(weather, min_temp20, on='FIPSYearMonth')\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21fb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the max temp dataframe\n",
    "weather = pd.merge(weather, max_temp20, on='FIPSYearMonth')\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearranging the columns\n",
    "weather = weather[['FIPSYearMonth', 'FIPS', 'Year', 'Month', 'precip', 'mintemp', 'maxtemp', 'avgtemp']]\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552796e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac1af8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#double checking in case we creating null values\n",
    "weather.isnull().sum()\n",
    "#bless the NOAA and their impeccably complete data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffe2b8",
   "metadata": {},
   "source": [
    "All of the weather data is clean and the 20 year period we will be focusing on is assembled into the dataframe, **weather**. Now to export the four whole dataframes and the **weather** dataframe to fresh csv files. To save drive space, we can now zip the old data, move it to a storage drive and leave ourselves a note up at the top of the notebook so we can find it easily. Same goes for the cleaned data that spans back to 1895-- we likely wont use it in this iteration of the project but we can put it on ice (...compress it) in case we need to use it later.\n",
    "\n",
    "We can remake the FIPSYearMonth column for each of these if need be, so prior to exporting we can save a bit of space and drop that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54039aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the FIPSYearMonth column from\n",
    "#precip = precip.drop(columns=['FIPSYearMonth'])\n",
    "#avg_temp = avg_temp.drop(columns=['FIPSYearMonth'])\n",
    "#min_temp = min_temp.drop(columns=['FIPSYearMonth'])\n",
    "#max_temp = max_temp.drop(columns=['FIPSYearMonth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af598222",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spit out the full cleaned dataframes and the combined weather dataframe\n",
    "#avg_temp.to_csv('avg_temp.csv', index=False)\n",
    "#min_temp.to_csv('min_temp.csv', index=False)\n",
    "#max_temp.to_csv('max_temp.csv', index=False)\n",
    "#precip.to_csv('precip.csv', index=False)\n",
    "#weather.to_csv('weather.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
